-- Seed papers
INSERT INTO public.study_notes (title, author, year, type, completed, notes, sort_order) VALUES
  ('Attention Is All You Need', 'Vaswani et al.', '2017', 'paper', FALSE, '', 1),
  ('The Illustrated Transformer', 'Jay Alammar', '2018', 'paper', FALSE, '', 2),
  ('BERT: Pre-training of Deep Bidirectional Transformers', 'Devlin et al.', '2018', 'paper', FALSE, '', 3),
  ('Language Models are Few-Shot Learners (GPT-3)', 'Brown et al.', '2020', 'paper', FALSE, '', 4),
  ('Scaling Laws for Neural Language Models', 'Kaplan et al.', '2020', 'paper', FALSE, '', 5),
  ('Training Compute-Optimal Large Language Models (Chinchilla)', 'Hoffmann et al.', '2022', 'paper', FALSE, '', 6),
  ('LLaMA: Open and Efficient Foundation Language Models', 'Touvron et al.', '2023', 'paper', FALSE, '', 7),
  ('RoFormer: Rotary Position Embedding', 'Su et al.', '2021', 'paper', FALSE, '', 8),
  ('FlashAttention', 'Dao et al.', '2022', 'paper', FALSE, '', 9),
  ('Retrieval-Augmented Generation (RAG)', 'Lewis et al.', '2020', 'paper', FALSE, '', 10),
  ('Training Language Models to Follow Instructions with Human Feedback (InstructGPT)', 'Ouyang et al.', '2022', 'paper', FALSE, '', 11),
  ('Direct Preference Optimization (DPO)', 'Rafailov et al.', '2023', 'paper', FALSE, '', 12),
  ('Chain-of-Thought Prompting Elicits Reasoning in Large Language Models', 'Wei et al.', '2022', 'paper', FALSE, '', 13),
  ('ReAct: Reasoning and Acting', 'Yao et al.', '2022 / ICLR 2023', 'paper', FALSE, '', 14),
  ('DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning', 'Guo et al.', '2025', 'paper', FALSE, '', 15),
  ('Qwen3 Technical Report', 'Yang et al.', '2025', 'paper', FALSE, '', 16),
  ('Outrageously Large Neural Networks: Sparsely-Gated Mixture of Experts', 'Shazeer et al.', '2017', 'paper', FALSE, '', 17),
  ('Switch Transformers', 'Fedus et al.', '2021', 'paper', FALSE, '', 18),
  ('Mixtral of Experts', 'Mistral AI', '2024', 'paper', FALSE, '', 19),
  ('Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints', 'Komatsuzaki et al.', '2022 / ICLR 2023', 'paper', FALSE, '', 20),
  ('The Platonic Representation Hypothesis', 'Huh et al.', '2024', 'paper', FALSE, '', 21),
  ('Textbooks Are All You Need', 'Gunasekar et al.', '2023', 'paper', FALSE, '', 22),
  ('Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet', 'Templeton et al.', '2024', 'paper', FALSE, '', 23),
  ('PaLM: Scaling Language Modeling with Pathways', 'Chowdhery et al.', '2022', 'paper', FALSE, '', 24),
  ('GLaM: Generalist Language Model', 'Du et al.', '2022', 'paper', FALSE, '', 25),
  ('The Smol Training Playbook', 'Hugging Face', '2025', 'paper', FALSE, '', 26);
